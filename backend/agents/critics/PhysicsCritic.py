import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from collections import deque
import json

@dataclass
class CriticReport:
    """Structured report from the critic agent."""
    timestamp: float
    overall_performance: float  # 0-1 score
    gate_alignment: float  # How well gate decisions match expectations
    error_distribution: Dict[str, float]
    recommendations: List[str]
    failure_modes: List[str]
    gate_statistics: Dict[str, float]
    confidence: float  # Critic's confidence in its assessment
    
    def to_dict(self):
        return {
            "timestamp": self.timestamp,
            "overall_performance": self.overall_performance,
            "gate_alignment": self.gate_alignment,
            "error_distribution": self.error_distribution,
            "recommendations": self.recommendations,
            "failure_modes": self.failure_modes,
            "gate_statistics": self.gate_statistics,
            "confidence": self.confidence
        }

class PhysicsCritic:
    """
    A meta-agent that monitors and evaluates a gated hybrid agent.
    
    The Critic Agent:
    1. Tracks prediction accuracy vs ground truth
    2. Analyzes gate decision patterns
    3. Identifies failure modes and edge cases
    4. Suggests when retraining is needed
    5. Validates that the gate is making sensible decisions
    
    Philosophy: The critic operates on a different timescale than the agent.
    While the agent makes real-time predictions, the critic performs periodic
    analysis to ensure the agent's decision-making remains sound.
    """
    
    def __init__(self, 
                 window_size: int = 100,
                 error_threshold: float = 0.1,
                 gate_alignment_threshold: float = 0.8):
        """
        Args:
            window_size: Number of recent predictions to analyze
            error_threshold: Max acceptable error rate before flagging
            gate_alignment_threshold: Min expected gate alignment score
        """
        self.window_size = window_size
        self.error_threshold = error_threshold
        self.gate_alignment_threshold = gate_alignment_threshold
        
        # Rolling history of observations
        self.prediction_history = deque(maxlen=window_size)
        self.ground_truth_history = deque(maxlen=window_size)
        self.gate_history = deque(maxlen=window_size)
        self.input_history = deque(maxlen=window_size)
        
        # Performance metrics
        self.total_evaluations = 0
        self.critical_failures = 0
        
    def observe(self, 
                input_state: any,
                prediction: any,
                ground_truth: float,
                gate_value: float):
        """
        Record a single agent decision.
        Supports both Legacy (Array) and Universal (Dict) inputs.
        """
        # Normalize Input to Dict if possible
        normalized_input = input_state
        if isinstance(input_state, np.ndarray):
            # Legacy Flight Wrapper: [mass, velocity, altitude] usually
            # We wrap it to preserve data
            normalized_input = {
                "physics_domain": "FLIGHT",
                "legacy_array": input_state.tolist(),
                "velocity": float(input_state[1]) if len(input_state) > 1 else 0.0
            }
        
        # Normalize Prediction to Dict if it's a scalar (Legacy Flight)
        normalized_pred = prediction
        if isinstance(prediction, (float, int, np.floating, np.integer)):
             normalized_pred = {"physics_domain": "FLIGHT", "value": float(prediction)}
             
        self.prediction_history.append(normalized_pred)
        self.ground_truth_history.append(ground_truth)
        self.gate_history.append(gate_value)
        self.input_history.append(normalized_input) # Store normalized
    def analyze(self) -> CriticReport:
        """
        Perform comprehensive analysis of recent agent behavior.
        """
        # Universal Safety Check: We should check violations even with 1 sample.
        # But statistical metrics need more data.
        
        # Determine if inputs are Dicts (Universal) or Arrays (Legacy Flight)
        is_dict_input = False
        if len(self.input_history) > 0 and isinstance(self.input_history[0], dict):
             is_dict_input = True

        failure_modes = []
        
        # --- UNIVERSAL CHECK (Immediate) ---
        if len(self.prediction_history) > 0:
             # Check for fundamental physics violations regardless of sample size
             univ_violations = self._check_conservation_laws(None, self.prediction_history)
             failure_modes.extend(univ_violations)

        # Statistical Analysis (Needs > 10 samples)
        if len(self.prediction_history) < 10:
            return CriticReport(
                timestamp=self.total_evaluations,
                overall_performance=0.0,
                gate_alignment=0.0, 
                error_distribution={}, 
                recommendations=["Insufficient data for stats"] if not failure_modes else ["See Critical Failures"], 
                failure_modes=list(set(failure_modes)), # Return detected violations!
                gate_statistics={}, 
                confidence=0.1
            )
        
        # ... Rest of Statistical Analysis from previous logic ...
        # Only proceed if numerical inputs (Legacy Flight Mode)
        if not is_dict_input:
            # Reconstruct numpy arrays for legacy stats
            # Warning: this fails if even one input is a dict. Logic below implies all or nothing.
            try:
                predictions = np.array(self.prediction_history)
                ground_truth = np.array(self.ground_truth_history)
                gates = np.array(self.gate_history)
                inputs = np.array(self.input_history) # This handles arrays
            except Exception:
                # Fallback if mixed types
                return CriticReport(timestamp=self.total_evaluations, overall_performance=0.5, gate_alignment=0, error_distribution={}, recommendations=[], failure_modes=failure_modes, gate_statistics={}, confidence=0.0)

            # 1. PERFORMANCE ANALYSIS
            errors = np.abs(predictions - ground_truth)
            relative_errors = errors / (ground_truth + 1e-6)
            overall_performance = 1.0 - np.clip(np.mean(relative_errors), 0, 1) # Simple score
            
            # 2. Gate/Velocity Analysis
            velocities = inputs[:, 1]
            
            # Re-call _detect for other stats-based failures
            stats_failures = self._detect_failure_modes(errors, relative_errors, gates, velocities, inputs)
            failure_modes.extend(stats_failures)
            
            # Compute stats results... (Simplified placeholders for this patch)
            error_dist = {"mean_error": float(np.mean(errors))}
            gate_stats = {"mean_gate": float(np.mean(gates))}
            gate_align = 0.5 # Placeholder
            recs = [] # Placeholder
            conf = 0.8
        else:
            # Universal Mode Stats (Not implemented for this ticket, we focus on Safety Violations)
            overall_performance = 1.0 # Assume ok unless violated
            error_dist = {}
            gate_stats = {}
            gate_align = 0.0
            recs = []
            conf = 0.5

        return CriticReport(
            timestamp=self.total_evaluations,
            overall_performance=overall_performance,
            gate_alignment=gate_align,
            error_distribution=error_dist,
            recommendations=recs,
            failure_modes=list(set(failure_modes)), # Unique
            gate_statistics=gate_stats,
            confidence=conf
        )

    def _check_conservation_laws(self, inputs: any, predictions: deque) -> List[str]:
        """
        Validates fundamental physics laws across ALL domains.
        """
        violations = []
        if len(predictions) == 0: return []
        
        # Latest Sample
        last_pred = self.prediction_history[-1]
        last_input = self.input_history[-1]
        
        # Handle Legacy/Array inputs gracefully
        if not isinstance(last_input, dict):
             # Should be handled by observe normalization or ignored for Universal Checks
             return []
             
        domain = last_input.get("physics_domain", "FLIGHT")
        
        # --- 1. THERMODYNAMICS ---
        if domain == "THERMODYNAMICS":
            # 1st Law: Eff <= 1.0
            eff = last_pred.get("efficiency", 0.0)
            if eff > 1.0:
                 violations.append(f"THERMO VIOLATION: Efficiency {eff:.2f} > 1.0 (1st Law)")
            
            # 2nd Law: Eff <= Carnot (1 - Tc/Th)
            t_hot = last_input.get("t_hot", 0)
            t_cold = last_input.get("t_cold", 0)
            if t_hot > t_cold and t_hot > 0:
                carnot = 1.0 - (t_cold / t_hot)
                if eff > carnot + 0.01: # 1% tolerance
                     violations.append(f"THERMO VIOLATION: Efficiency {eff:.2f} > Carnot Limit {carnot:.2f} (2nd Law)")

        # --- 2. OPTICS ---
        elif domain == "OPTICS":
            r = last_pred.get("reflectance", 0.0)
            t = last_pred.get("transmittance", 0.0)
            a = last_pred.get("absorbance", 0.0)
            # Only check if all three are provided or inferred
            if "reflectance" in last_pred: 
                total = r + t + a
                # If total deviates significantly from 1.0 (and material is not gain medium/laser)
                if abs(total - 1.0) > 0.01 and not last_input.get("active_gain", False):
                     violations.append(f"OPTICS VIOLATION: Energy Conservation (Sum={total:.3f} != 1.0)")

        # --- 3. QUANTUM ---
        elif domain == "QUANTUM":
            probs = last_pred.get("probabilities", [])
            if probs:
                prob_sum = sum(probs)
                if abs(prob_sum - 1.0) > 0.01:
                    violations.append(f"QUANTUM VIOLATION: Unitary Norm (Sum={prob_sum:.3f})")

        # --- 4. RELATIVITY & ASTROPHYSICS ---
        elif domain in ["RELATIVITY", "ASTROPHYSICS"]:
            v = last_pred.get("velocity_m_s", last_input.get("velocity", 0.0))
            c = 299792458
            if v > c:
                 violations.append(f"RELATIVITY VIOLATION: Superluminal Velocity ({v:.2e} > c)")
            
            # Astrophysics: Kepler's 3rd Law Consistency? (T^2 ~ a^3)
            # Heuristic check if properorbital params present
            pass

        # --- 5. FLUIDS & ACOUSTICS ---
        elif domain in ["FLUID", "ACOUSTICS"]:
            m_in = last_input.get("mass_flow_in", 0.0)
            m_out = last_pred.get("mass_flow_out", 0.0)
            if abs(m_in - m_out) > 1e-5 and last_input.get("steady_state"):
                violations.append(f"FLUID VIOLATION: Mass Continuity Mismatch")
            
            # Acoustics: SPL cannot imply pressure > ambient vacuum limit (physically rare but impossible)
            # Pressure Amplitude < Ambient Pressure (for linear acoustics, though non-linear shock waves exist)
            pass

        # --- 6. ELECTROMAGNETISM & PLASMA ---
        elif domain in ["ELECTROMAGNETISM", "PLASMA", "CIRCUITS"]:
            # Charge Conservation / Kirchhoff
            if domain == "CIRCUITS":
                 # Power Balance: Supply >= Demand
                 # Need to safely get supply, demand
                 supply = last_pred.get("supply_power", 0)
                 demand = last_pred.get("demand_power", 0)
                 margin = last_pred.get("margin", 0)
                 # Check logic: Margin ~= Supply - Demand
                 if abs((supply - demand) - margin) > 0.1:
                      violations.append("CIRCUIT VIOLATION: KCL Power Mismatch")
            
            if domain == "PLASMA":
                 # Quasi-Neutrality check (n_e ~ n_i * Z)
                 n_e = last_pred.get("electron_density", 0)
                 n_i = last_pred.get("ion_density", 0)
                 Z = last_input.get("Z", 1)
                 if n_e > 0 and abs(n_e - n_i * Z) / n_e > 0.1 and not last_input.get("non_neutral"):
                      violations.append("PLASMA VIOLATION: Quasi-Neutrality Broken")

        # --- 7. NUCLEAR ---
        elif domain == "NUCLEAR":
             # Criticality Checks
             k = last_pred.get("k_eff", 0)
             prompt_crit = last_pred.get("prompt_critical", False)
             if prompt_crit:
                  violations.append("NUCLEAR SAFETY: Prompt Criticality Detected")

        return violations
    
    def _generate_recommendations(self,
                                 overall_performance: float,
                                 gate_alignment: float,
                                 error_distribution: Dict[str, float],
                                 gate_statistics: Dict[str, float],
                                 failure_modes: List[str]) -> List[str]:
        """Generate actionable recommendations based on analysis."""
        recommendations = []
        
        # Performance-based recommendations
        if overall_performance < 0.7:
            recommendations.append("âš ï¸ RETRAIN: Overall performance below acceptable threshold")
        
        if gate_alignment < self.gate_alignment_threshold:
            recommendations.append("ðŸ”§ GATE ISSUE: Gate decisions misaligned with expected behavior")
        
        # Error distribution recommendations
        if error_distribution["low_speed_error"] > error_distribution["high_speed_error"] * 2:
            recommendations.append("ðŸ§® CHECK PHYSICS: Low-speed errors unusually high (physics heuristic may be wrong)")
        
        if error_distribution["high_speed_error"] > error_distribution["low_speed_error"] * 2:
            recommendations.append("ðŸ§  TRAIN NEURAL: High-speed errors dominate (neural branch needs more data)")
        
        # Gate statistics recommendations
        if gate_statistics["std_gate"] < 0.1:
            recommendations.append("ðŸŽ›ï¸ GATE FROZEN: Gate not adapting to different scenarios")
        
        if gate_statistics["low_speed_gate"] > 0.5:
            recommendations.append("ðŸ“‰ GATE MISCALIBRATED: Trusting neural too much at low speeds")
        
        if gate_statistics["high_speed_gate"] < 0.5:
            recommendations.append("ðŸ“ˆ GATE MISCALIBRATED: Trusting physics too much at high speeds")
        
        # Failure mode recommendations
        if "concept drift" in str(failure_modes):
            recommendations.append("ðŸ”„ CONCEPT DRIFT: Environment may have changed, consider online learning")
        
        if "numerical instability" in str(failure_modes):
            recommendations.append("âš¡ STABILITY ISSUE: Add gradient clipping or normalization")
        
        # If everything is good, say so!
        if not recommendations and not failure_modes:
            recommendations.append("âœ… NOMINAL: Agent performing within expected parameters")
        
        return recommendations
    
    def should_evolve(self) -> Tuple[bool, str, str]:
        """
        Critical decision: Should the agent be evolved/retrained?
        
        Returns:
            (should_evolve, reason, strategy)
        """
        if len(self.prediction_history) < self.window_size // 2:
            return False, "Insufficient data for retrain decision", None
        
        report = self.analyze()
        
        # Critical failure: Immediate retrain
        if report.overall_performance < 0.5:
            self.critical_failures += 1
            return True, f"Performance critically low: {report.overall_performance:.2f}", "RETRAIN_PHYSICS_SURROGATE"
        
        # Gate failure: Retrain needed
        if report.gate_alignment < 0.5:
            return True, f"Gate severely misaligned: {report.gate_alignment:.2f}", "RETUNE_GATE"
        
        # Multiple failure modes: Retrain
        if len(report.failure_modes) >= 3:
            return True, f"Multiple failure modes detected: {len(report.failure_modes)}", "FULL_RETRAIN"
        
        # Concept drift: Retrain
        if any("drift" in mode.lower() for mode in report.failure_modes):
            return True, "Concept drift detected", "ONLINE_ADAPTATION"
        
        return False, "Agent within acceptable parameters", None
    
    def generate_training_suggestions(self) -> Dict[str, any]:
        """
        Suggest training data or hyperparameter adjustments.
        
        Returns:
            Dictionary with training suggestions
        """
        if len(self.input_history) < 10:
            return {"message": "Need more data for suggestions"}
        
        inputs = np.array(self.input_history)
        errors = np.abs(np.array(self.prediction_history) - np.array(self.ground_truth_history))
        
        # Find error hotspots in input space
        velocities = inputs[:, 1]
        
        # Where are errors highest?
        high_error_indices = np.argsort(errors)[-10:]  # Top 10 errors
        problematic_velocities = velocities[high_error_indices]
        
        suggestions = {
            "focus_regions": {
                "velocity_range": [float(np.min(problematic_velocities)), 
                                  float(np.max(problematic_velocities))],
                "mean_problematic_velocity": float(np.mean(problematic_velocities))
            },
            "recommended_samples": int(50),  # Generate 50 more samples in this region
            "hyperparameter_suggestions": {}
        }
        
        # If gate variance is too low, suggest higher learning rate for gate
        if np.std(self.gate_history) < 0.1:
            suggestions["hyperparameter_suggestions"]["gate_learning_rate"] = "increase by 2x"
        
        # If errors are high overall, suggest more training epochs
        if np.mean(errors) > 20:
            suggestions["hyperparameter_suggestions"]["training_epochs"] = "increase by 50%"
        
        return suggestions
    
    def evolve_agent(self, agent: 'PhysicsAgent') -> Dict[str, any]:
        """
        Trigger Deep Evolution (Neural Training) for PhysicsSurrogate.
        Constraint: Only trains if we have high-quality Ground Truth from Oracle/Teacher.
        """
        training_batch = []
        
        # We need at least some history
        if len(self.input_history) < 10:
             return {"status": "skipped", "reason": "Insufficient history"}
             
        # Convert deque to list for indexing
        inputs = list(self.input_history)
        truths = list(self.ground_truth_history)
        preds = list(self.prediction_history)
        
        # Iterate and select samples where Error is High (Active Learning)
        # OR select all samples where we have Ground Truth (Teacher Forcing)
        
        for i in range(len(inputs)):
            x = inputs[i]
            y_truth = truths[i]
            y_pred = preds[i]
            
            # PhysicsSurrogate inputs are: [mass/1000.0, g/9.81, rho/1.225, area/10.0, 1.0]
            # But observe() stores RAW input state [mass, velocity, altitude] usually.
            # We need to map observed state to Surrogate Input Format.
            # This mapping depends on how observe() was called.
            # Assuming observe() received the same raw params.
            
            # For now, let's assume the Critic stores RAW inputs in compatible format 
            # OR we reconstruct it.
            # Simplification: The verification script passed raw inputs in a way we can use.
            # But PhysicsAgent._solve_flight_dynamics uses internal logic to build the vector.
            
            # If we can't perfectly reconstruct the input vector here, we might need 
            # the Agent to log the "neural input vector" to the Critic.
            # For this MVP, we will rely on the Critic having decent data or skip.
            
            # Let's assume 'x' is [mass, g, rho, area, bias] if passed correctly.
            # If not, we skip.
            if len(x) == 5: 
                 training_batch.append((x, [y_truth, 0.0])) # y_truth is Thrust, 2nd output Speed logic needed?
                 # Wait, Surrogate outputs [Thrust, Speed]. 
                 # Ground Truth passed to observe() is usually just one value (Thrust).
                 # This impedance mismatch suggests we need a richer observe() or specific adaptation.
                 
                 # PATCH: For Tier 3.5, we trust the verification script's approach 
                 # which passes correct tuples. In production, we'd need a transformer.
                 pass
        
        # If we have valid batches (mocked or real)
        if len(training_batch) > 0:
             return agent.evolve(training_batch)
             
        return {"status": "skipped", "reason": "No valid training pairs identified"}

    def reset(self):
        """Clear history and start fresh analysis."""
        self.prediction_history.clear()
        self.ground_truth_history.clear()
        self.gate_history.clear()
        self.input_history.clear()
        
    def export_report(self, filepath: str):
        """Export analysis report to JSON."""
        report = self.analyze()
        with open(filepath, 'w') as f:
            json.dump(report.to_dict(), f, indent=2)
